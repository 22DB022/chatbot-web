"""
ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å­¦ç¿’ã‚¢ãƒ—ãƒªï¼ˆWebç‰ˆï¼‰
æ—¢å­˜ã®RAGãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ãŸFlask API
PostgreSQL/MySQL/SQLite 3ç¨®å¯¾å¿œ
"""
from flask import Flask, request, jsonify, render_template, send_file, send_from_directory
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from openai import OpenAI
import os
import json
import numpy as np
from datetime import datetime
from dotenv import load_dotenv
import sqlite3

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# PyMySQL (MySQLç”¨)
try:
    import pymysql
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False

# Flaskã‚¢ãƒ—ãƒªåˆæœŸåŒ–
app = Flask(__name__, 
            template_folder='templates',
            static_folder='assets',
            static_url_path='/assets')

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™
limiter = Limiter(
    app=app,
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"]
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
db = None
client = None
conversation_history = {}


class RAGDatabase:
    """RAGå¯¾å¿œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆPostgreSQL/MySQL/SQLite 3ç¨®å¯¾å¿œï¼‰"""
    
    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’å–å¾—
        self.db_url = os.getenv('DATABASE_URL')  # PostgreSQL (Supabase)
        self.db_name = os.getenv('DB_NAME')       # MySQL (XAMPP)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        if self.db_url:
            # PostgreSQL (Supabase) - æœ¬ç•ªç’°å¢ƒ
            print("âœ… Supabase PostgreSQLæ¥ç¶š")
            self.db_type = 'postgresql'
        elif self.db_name and MYSQL_AVAILABLE:
            # MySQL (XAMPP) - ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º
            self.db_config = {
                'host': os.getenv('DB_HOST', 'localhost'),
                'user': os.getenv('DB_USER', 'root'),
                'password': os.getenv('DB_PASSWORD', ''),
                'database': self.db_name,
                'charset': 'utf8mb4'
            }
            print(f"âœ… MySQLæ¥ç¶šè¨­å®šå®Œäº†: {self.db_name}")
            self.db_type = 'mysql'
        else:
            # SQLite - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.db_path = "rag_study_data.db"
            print(f"âš ï¸ SQLiteãƒ¢ãƒ¼ãƒ‰: {self.db_path}")
            self.db_type = 'sqlite'
    
    def get_connection(self):
        """DBæ¥ç¶šã‚’å–å¾—"""
        if self.db_type == 'postgresql':
            import psycopg2
            return psycopg2.connect(self.db_url)
        elif self.db_type == 'mysql':
            return pymysql.connect(**self.db_config)
        else:  # sqlite
            import sqlite3
            return sqlite3.connect(self.db_path)
    
    def vector_search(self, query_embedding, top_k=5):
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰"""
        conn = self.get_connection()
        
        try:
            if self.db_type == 'postgresql':
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT filename, chunk_text, embedding, page_number 
                    FROM pdf_contents
                """)
            elif self.db_type == 'mysql':
                cursor = conn.cursor(pymysql.cursors.DictCursor)
                cursor.execute("""
                    SELECT filename, chunk_text, embedding, page_number 
                    FROM pdf_contents
                """)
            else:  # sqlite
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT filename, chunk_text, embedding, page_number 
                    FROM pdf_contents
                """)
            
            results = cursor.fetchall()
            cursor.close()
            
            if not results:
                return []
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
            query_vec = np.array(query_embedding)
            similarities = []
            
            for row in results:
                try:
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦åˆ—ã«ã‚¢ã‚¯ã‚»ã‚¹
                    if self.db_type == 'postgresql':
                        chunk_embedding = json.loads(row[2])
                        filename = row[0]
                        chunk_text = row[1]
                        page_number = row[3]
                    elif self.db_type == 'mysql':
                        chunk_embedding = json.loads(row['embedding'])
                        filename = row['filename']
                        chunk_text = row['chunk_text']
                        page_number = row['page_number']
                    else:  # sqlite
                        chunk_embedding = json.loads(row['embedding'])
                        filename = row['filename']
                        chunk_text = row['chunk_text']
                        page_number = row['page_number']
                    
                    chunk_vec = np.array(chunk_embedding)
                    
                    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                    similarity = np.dot(query_vec, chunk_vec) / (
                        np.linalg.norm(query_vec) * np.linalg.norm(chunk_vec)
                    )
                    
                    similarities.append({
                        'filename': filename,
                        'text': chunk_text,
                        'page': page_number,
                        'similarity': float(similarity)
                    })
                except Exception as e:
                    continue
            
            # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            
            return similarities[:top_k]
            
        finally:
            conn.close()
    
    def get_pdf_list(self):
        """ç™»éŒ²æ¸ˆã¿PDFä¸€è¦§"""
        conn = self.get_connection()
        
        try:
            if self.db_type == 'postgresql':
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT filename, page_count, total_chars, total_chunks, added_date 
                    FROM pdf_metadata 
                    ORDER BY added_date DESC
                """)
                columns = ['filename', 'page_count', 'total_chars', 'total_chunks', 'added_date']
                results = [dict(zip(columns, row)) for row in cursor.fetchall()]
            elif self.db_type == 'mysql':
                cursor = conn.cursor(pymysql.cursors.DictCursor)
                cursor.execute("""
                    SELECT filename, page_count, total_chars, total_chunks, added_date 
                    FROM pdf_metadata 
                    ORDER BY added_date DESC
                """)
                results = [dict(row) for row in cursor.fetchall()]
            else:  # sqlite
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT filename, page_count, total_chars, total_chunks, added_date 
                    FROM pdf_metadata 
                    ORDER BY added_date DESC
                """)
                results = [dict(row) for row in cursor.fetchall()]
            
            cursor.close()
            return results
            
        finally:
            conn.close()
    
    def get_stats(self):
        """çµ±è¨ˆæƒ…å ±"""
        conn = self.get_connection()
        
        try:
            cursor = conn.cursor()
        
            cursor.execute("""
            SELECT 
                COUNT(*) as pdf_count,
                COALESCE(SUM(page_count), 0) as total_pages,
                COALESCE(SUM(total_chunks), 0) as total_chunks
            FROM pdf_metadata
         """)
        
            result = cursor.fetchone()
            cursor.close()
        
         # å…¨ã¦ã®DBã‚¿ã‚¤ãƒ—ã§ã‚¿ãƒ—ãƒ«ãŒè¿”ã‚‹ã®ã§çµ±ä¸€å‡¦ç†
            return {
            'pdf_count': result[0],
            'total_pages': result[1],
            'total_chunks': result[2]
         }
            
        finally:
            conn.close()


BASE_SYSTEM_PROMPT = """# å‰ææ¡ä»¶
- ã‚ãªãŸã¯ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æ¤œå®šãƒ™ãƒ¼ã‚·ãƒƒã‚¯å¯¾ç­–ã®æ•™è‚²AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ¤œå®šåˆæ ¼ã‚’ç›®æŒ‡ã™å­¦ç¿’è€…ã§ã™
- çµ¶å¯¾ã«æä¾›ã•ã‚ŒãŸå­¦ç¿’è³‡æ–™ã®ã¿ã‚’å‚ç…§ã—ã¦å›ç­”ã—ã¾ã™

# åˆ¶ç´„æ¡ä»¶
- å­¦ç¿’è³‡æ–™ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„å†…å®¹ã¯ã€Œè³‡æ–™ã«è¨˜è¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã‚‹
- è³‡æ–™ã«åŸºã¥ã‹ãªã„æ¨æ¸¬ã‚„å‰µä½œã¯ç¦æ­¢
- è¦ªã—ã¿ã‚„ã™ãåŠ±ã¾ã™å£èª¿ï¼ˆã€œã ã‚ˆã€ã€œã—ã¦ã¿ã‚ˆã†ã€ã‚ˆãã§ããŸã­ï¼‰
- å›ç­”ã¯400æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹
- å¿…ãšå‚ç…§ã—ãŸè³‡æ–™ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ç¤ºã™
- æ¤œå®šã«ç„¡é–¢ä¿‚ãªå†…å®¹ã«ã¯ä¸å¯§ã«æ–­ã‚‹

# ç”»åƒè¡¨ç¤ºã«ã¤ã„ã¦
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»¥ä¸‹ã®ã‚ˆã†ãªè¦æ±‚ã‚’ã—ãŸå ´åˆã€è©²å½“ãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’è¡¨ç¤ºã™ã‚‹ï¼š
- ã€Œå›³ã‚’è¦‹ã›ã¦ã€ã€Œç”»åƒã‚’è¡¨ç¤ºã—ã¦ã€
- ã€Œå›³XXã€ã€Œå›³è¡¨ã€ã€Œã‚¤ãƒ©ã‚¹ãƒˆã€ãªã©ã®è¨€åŠ
- ã€Œè¦–è¦šçš„ã«è¦‹ãŸã„ã€ã€Œè¦‹ã›ã¦ã»ã—ã„ã€

å¯¾å¿œæ–¹æ³•: [IMAGE:ãƒ•ã‚¡ã‚¤ãƒ«å|ãƒšãƒ¼ã‚¸ç•ªå·] ã®å½¢å¼ã§æŒ‡å®š

# å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³

## ãƒ‘ã‚¿ãƒ¼ãƒ³1: çŸ¥è­˜ãƒ»èª¬æ˜ã‚’æ±‚ã‚ã‚‰ã‚ŒãŸå ´åˆ
1. è³‡æ–™ã®å†…å®¹ã‚’åŸºã«æ˜ç¢ºã«èª¬æ˜ã™ã‚‹
2. å…·ä½“ä¾‹ã‚’1ã€œ2å€‹æŒ™ã’ã‚‹
3. ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æ˜è¨˜ã™ã‚‹
4. è»½ãã€Œç†è§£ã§ããŸã‹ãªï¼Ÿã€ã¨ç¢ºèª
5. è³‡æ–™ã‹ã‚‰ã®æƒ…å ±ã®ã¿ã§å›ç­”

## ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç†è§£ã‚’ç¤ºã—ãŸå ´åˆï¼ˆâ˜…é‡è¦â˜…ï¼‰
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»¥ä¸‹ã®ã‚ˆã†ãªç™ºè¨€ã‚’ã—ãŸå ´åˆã€**å¿…ãšç†è§£åº¦ç¢ºèªã®å•é¡Œã‚’å‡ºé¡Œã™ã‚‹**ï¼š

### ç†è§£ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ•ãƒ¬ãƒ¼ã‚ºï¼š
- ã€Œåˆ†ã‹ã£ãŸã€ã€Œã‚ã‹ã£ãŸã€ã€Œç†è§£ã—ãŸã€
- ã€Œãªã‚‹ã»ã©ã€ã€Œãã†ã„ã†ã“ã¨ã‹ã€ã€Œãã†ãªã‚“ã ã€
- ã€Œåˆ†ã‹ã‚Šã¾ã—ãŸã€ã€Œç†è§£ã§ãã¾ã—ãŸã€
- ã€ŒOKã€ã€Œäº†è§£ã€ã€Œå¤§ä¸ˆå¤«ã€
- ã€Œç°¡å˜ã ã­ã€ã€Œè¦šãˆãŸã€
- ã€Œã‚ã‚ŠãŒã¨ã†ã€ï¼ˆèª¬æ˜ã®å¾Œï¼‰

### å¯¾å¿œæ‰‹é †ï¼š
1. ã¾ãšç†è§£ã‚’èªã‚ã‚‹ï¼ˆã€Œã‚ˆã—ï¼ã€ã€Œã„ã„ã­ï¼ã€ï¼‰
2. **å³åº§ã«**ã€Œã˜ã‚ƒã‚ã€æœ¬å½“ã«ç†è§£ã§ããŸã‹ç¢ºèªã—ã¦ã¿ã‚ˆã†ï¼ã€ã¨è¨€ã†
3. **å¿…ãšå•é¡Œã‚’1å•å‡ºé¡Œã™ã‚‹**ï¼ˆ3æŠå•é¡Œï¼‰
4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’å¾…ã¤

## ãƒ‘ã‚¿ãƒ¼ãƒ³3: å•é¡Œãƒ»æ¼”ç¿’ã‚’æ±‚ã‚ã‚‰ã‚ŒãŸå ´åˆ
1. å­¦ç¿’å†…å®¹ã«æ²¿ã£ãŸé¸æŠå¼å•é¡Œã‚’1å•å‡ºé¡Œï¼ˆAã€Bã€Cã®3æŠï¼‰
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã‚’å¾…ã¤
3. å›ç­”å¾Œã€ä»¥ä¸‹ã®å¯¾å¿œï¼š
   - **æ­£è§£ã®å ´åˆ**: è¤’ã‚ã¦ã€è§£èª¬ã‚’è¿½åŠ ã—ã€æ¬¡ã®å­¦ç¿’ã‚’ææ¡ˆ
   - **ä¸æ­£è§£ã®å ´åˆ**: å¦å®šã›ãšã€Œæƒœã—ã„ï¼ã€ã¨åŠ±ã¾ã—ã€ãƒ’ãƒ³ãƒˆã‚’å‡ºã—ã¦å†æŒ‘æˆ¦ã‚’ä¿ƒã™
4. è³‡æ–™ã‹ã‚‰ã®å†…å®¹ã§å•é¡Œã‚’ä½œæˆ

## ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒ’ãƒ³ãƒˆã‚’æ±‚ã‚ã‚‰ã‚ŒãŸå ´åˆ
1. **1å›ç›®**: æ¦‚å¿µçš„ãªãƒ’ãƒ³ãƒˆï¼ˆã©ã®åˆ†é‡ã«é–¢ä¿‚ã™ã‚‹ã‹ï¼‰
2. **2å›ç›®**: ã‚ˆã‚Šå…·ä½“çš„ãªãƒ’ãƒ³ãƒˆï¼ˆé¸æŠè‚¢ã®çµã‚Šè¾¼ã¿ï¼‰
3. **3å›ç›®**: ã»ã¼ç­”ãˆã«è¿‘ã„æƒ…å ±ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç¤ºã™ï¼‰

## ãƒ‘ã‚¿ãƒ¼ãƒ³5: åˆå›æ¥è§¦ãƒ»æŒ¨æ‹¶
- ç°¡æ½”ã«æŒ¨æ‹¶
- ã€Œä½•ã‚’å­¦ç¿’ã—ãŸã„ï¼Ÿã€ã¨èã
- é¸æŠè‚¢ã¯å‡ºã•ãªã„ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±ãªè³ªå•ã‚’ä¿ƒã™ï¼‰

# ç¦æ­¢äº‹é …
- è³‡æ–™å¤–ã®æƒ…å ±ã‚’å‰µä½œã™ã‚‹
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç†è§£ã‚’ç¤ºã—ãŸã®ã«å•é¡Œã‚’å‡ºã•ãªã„
- é–“é•ã„ã‚’è²¬ã‚ã‚‹å£èª¿
- é€£ç¶šã—ã¦è¤‡æ•°ã®å•é¡Œã‚’å‡ºã™
- é•·ã™ãã‚‹èª¬æ˜ï¼ˆ400æ–‡å­—ã‚’å¤§å¹…ã«è¶…ãˆã‚‹ï¼‰
"""


def initialize():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
    global db, client
    
    # OpenAI APIåˆæœŸåŒ–
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise Exception("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    client = OpenAI(api_key=api_key)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ï¼ˆè‡ªå‹•åˆ¤å®šï¼‰
    db = RAGDatabase()
    
    if db.db_type == 'postgresql':
        print(f"âœ… RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº† (PostgreSQL - Supabase)")
    elif db.db_type == 'mysql':
        print(f"âœ… RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº† (MySQL)")
    else:
        print(f"âœ… RAGãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº† (SQLite - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)")


# ============================================
# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================

@app.route('/')
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return render_template('index.html')


@app.route('/api/health', methods=['GET'])
def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        stats = db.get_stats()
        return jsonify({
            'status': 'ok',
            'database': db.db_type.upper(),
            'stats': stats
        })
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/init', methods=['GET'])
def get_init_data():
    """åˆæœŸãƒ‡ãƒ¼ã‚¿å–å¾—"""
    try:
        stats = db.get_stats()
        pdf_list = db.get_pdf_list()
        
        return jsonify({
            'stats': stats,
            'pdf_list': pdf_list,
            'database_type': db.db_type.upper()
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/query', methods=['POST'])
@limiter.limit("10 per minute")
def query():
    """è³ªå•API - RAGãƒ­ã‚¸ãƒƒã‚¯"""
    try:
        data = request.json
        question = data.get('question')
        session_id = data.get('session_id', 'default')
        
        if not question:
            return jsonify({'error': 'è³ªå•ãŒç©ºã§ã™'}), 400
        
        stats = db.get_stats()
        if stats['pdf_count'] == 0:
            return jsonify({
                'answer': 'ã¾ã PDFè³‡æ–™ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚',
                'sources': [],
                'no_data': True
            })
        
        # ä¼šè©±å±¥æ­´ã‚’å–å¾—ã¾ãŸã¯åˆæœŸåŒ–
        if session_id not in conversation_history:
            conversation_history[session_id] = [
                {"role": "system", "content": BASE_SYSTEM_PROMPT}
            ]
        
        messages = conversation_history[session_id]
        
        # 1. è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        query_response = client.embeddings.create(
            model="text-embedding-3-small",
            input=question
        )
        query_embedding = query_response.data[0].embedding
        
        # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
        relevant_chunks = db.vector_search(query_embedding, top_k=5)
        
        if not relevant_chunks:
            return jsonify({
                'answer': 'é–¢é€£ã™ã‚‹æƒ…å ±ãŒè³‡æ–™ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚',
                'sources': []
            })
        
        # 3. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = "# é–¢é€£ã™ã‚‹å­¦ç¿’è³‡æ–™:\n\n"
        sources = []
        
        for i, chunk in enumerate(relevant_chunks, 1):
            context += f"ã€è³‡æ–™{i}: {chunk['filename']} ãƒšãƒ¼ã‚¸{chunk['page']}ã€‘\n"
            context += f"{chunk['text']}\n\n"
            
            sources.append({
                'filename': chunk['filename'],
                'page': chunk['page'],
                'similarity': round(chunk['similarity'], 3)
            })
        
        # 4. AIã«é€ä¿¡
        full_message = f"{context}\n# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™ã®ã¿ã‚’ä½¿ã£ã¦ã€å¿…ãšãƒšãƒ¼ã‚¸ç•ªå·ã‚’ç¤ºã—ãªãŒã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        
        messages.append({"role": "user", "content": full_message})
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        assistant_message = response.choices[0].message.content
        
        # ä¼šè©±å±¥æ­´ã‚’æ›´æ–°
        messages[-1] = {"role": "user", "content": question}
        messages.append({"role": "assistant", "content": assistant_message})
        
        # å±¥æ­´ç®¡ç†
        if len(messages) > 21:
            messages = [messages[0]] + messages[-20:]
        
        conversation_history[session_id] = messages
        
        return jsonify({
            'answer': assistant_message,
            'sources': sources
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/reset', methods=['POST'])
def reset_conversation():
    """ä¼šè©±å±¥æ­´ãƒªã‚»ãƒƒãƒˆ"""
    try:
        data = request.json
        session_id = data.get('session_id', 'default')
        
        if session_id in conversation_history:
            del conversation_history[session_id]
        
        return jsonify({'success': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/upload-pdf', methods=['POST'])
def upload_pdf():
    """PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ç™»éŒ²ï¼ˆAWS LambdaçµŒç”±ï¼‰"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        file = request.files['file']
        
        if file.filename == '':
            return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        if not file.filename.lower().endswith('.pdf'):
            return jsonify({'error': 'PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„'}), 400
        
        file.seek(0, 2)
        file_size = file.tell()
        file.seek(0)
        
        if file_size > 50 * 1024 * 1024:
            return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ï¼ˆæœ€å¤§50MBï¼‰'}), 400
        
        print(f"ğŸ“¤ PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        import base64
        pdf_data = file.read()
        pdf_base64 = base64.b64encode(pdf_data).decode('utf-8')
        
        # AWS Lambda URLã‚’å–å¾—
        lambda_url = os.getenv('AWS_LAMBDA_URL')
        
        if not lambda_url:
            return jsonify({'error': 'AWS_LAMBDA_URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 500
        
        print(f"ğŸš€ AWS Lambdaã«é€ä¿¡: {lambda_url}")
        
        import requests
        
        # Lambdaé–¢æ•°ã‚’å‘¼ã³å‡ºã—
        response = requests.post(
            lambda_url,
            json={
                'pdf_data': pdf_base64,
                'filename': file.filename
            },
            timeout=600  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        print(f"ğŸ“¨ Lambdaå¿œç­”: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            
            if result.get('success'):
                return jsonify({
                    'success': True,
                    'message': f'"{file.filename}" ã‚’ç™»éŒ²ã—ã¾ã—ãŸ',
                    'stats': {
                        'filename': result.get('filename'),
                        'page_count': result.get('page_count'),
                        'total_chars': result.get('total_chars'),
                        'total_chunks': result.get('total_chunks')
                    }
                })
            else:
                error_msg = result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')
                print(f"âŒ Lambda ã‚¨ãƒ©ãƒ¼: {error_msg}")
                return jsonify({'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}'}), 500
        else:
            error_text = response.text
            print(f"âŒ Lambda HTTPã‚¨ãƒ©ãƒ¼: {error_text}")
            return jsonify({'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_text}'}), 500
        
    except requests.exceptions.Timeout:
        print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        return jsonify({'error': 'å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚PDFãŒå¤§ãã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'}), 504
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500


@app.route('/api/images/<filename>/<int:page_number>', methods=['GET'])
def get_page_images(filename, page_number):
    """ç‰¹å®šãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’å–å¾—"""
    try:
        images = get_images_for_page(filename, page_number)
        
        # ç”»åƒãƒ‘ã‚¹ã‚’URLãƒ‘ã‚¹ã«å¤‰æ›
        for img in images:
            img['url'] = '/' + img['image_path'].replace('\\', '/')
        
        return jsonify({
            'images': images,
            'count': len(images)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# ============================================
# PDFå‡¦ç†é–¢æ•°
# ============================================

def process_pdf_file(pdf_path, filename):
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²"""
    import pdfplumber
    
    print(f"ğŸ“„ PDFå‡¦ç†é–‹å§‹: {filename}")
    
    pages_text = []
    total_chars = 0
    
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages, 1):
            text = page.extract_text()
            if text:
                text = text.strip()
                pages_text.append({'page': i, 'text': text})
                total_chars += len(text)
    
    if not pages_text:
        raise Exception("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    print(f"âœ… å…¨{len(pages_text)}ãƒšãƒ¼ã‚¸æŠ½å‡ºå®Œäº†")
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        if db.db_type == 'postgresql':
            cursor.execute("SELECT COUNT(*) FROM pdf_metadata WHERE filename = %s", (filename,))
        elif db.db_type == 'mysql':
            cursor.execute("SELECT COUNT(*) FROM pdf_metadata WHERE filename = %s", (filename,))
        else:  # sqlite
            cursor.execute("SELECT COUNT(*) FROM pdf_metadata WHERE filename = ?", (filename,))
        
        exists = cursor.fetchone()[0] > 0
        
        if exists:
            if db.db_type == 'postgresql':
                cursor.execute("DELETE FROM pdf_metadata WHERE filename = %s", (filename,))
                cursor.execute("DELETE FROM pdf_contents WHERE filename = %s", (filename,))
                cursor.execute("DELETE FROM pdf_images WHERE filename = %s", (filename,))
            elif db.db_type == 'mysql':
                cursor.execute("DELETE FROM pdf_metadata WHERE filename = %s", (filename,))
                cursor.execute("DELETE FROM pdf_contents WHERE filename = %s", (filename,))
                cursor.execute("DELETE FROM pdf_images WHERE filename = %s", (filename,))
            else:  # sqlite
                cursor.execute("DELETE FROM pdf_metadata WHERE filename = ?", (filename,))
                cursor.execute("DELETE FROM pdf_contents WHERE filename = ?", (filename,))
                cursor.execute("DELETE FROM pdf_images WHERE filename = ?", (filename,))
            conn.commit()
            print(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤: {filename}")
        
    finally:
        cursor.close()
    
    # ãƒãƒ£ãƒ³ã‚¯åŒ–ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    all_chunks = []
    
    for page_data in pages_text:
        chunks = chunk_text(page_data['text'])
        
        for chunk in chunks:
            embedding = create_embedding(chunk)
            all_chunks.append({
                'page': page_data['page'],
                'text': chunk,
                'embedding': embedding
            })
    
    print(f"âœ… å…¨{len(all_chunks)}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†å®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥
        if db.db_type == 'postgresql':
            cursor.execute("""
                INSERT INTO pdf_metadata 
                (filename, page_count, total_chars, total_chunks, added_date)
                VALUES (%s, %s, %s, %s, NOW())
            """, (filename, len(pages_text), total_chars, len(all_chunks)))
        elif db.db_type == 'mysql':
            cursor.execute("""
                INSERT INTO pdf_metadata 
                (filename, page_count, total_chars, total_chunks, added_date)
                VALUES (%s, %s, %s, %s, NOW())
            """, (filename, len(pages_text), total_chars, len(all_chunks)))
        else:  # sqlite
            cursor.execute("""
                INSERT INTO pdf_metadata 
                (filename, page_count, total_chars, total_chunks, added_date)
                VALUES (?, ?, ?, ?, ?)
            """, (filename, len(pages_text), total_chars, len(all_chunks), datetime.now().isoformat()))
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚’æŒ¿å…¥
        for chunk in all_chunks:
            embedding_json = json.dumps(chunk['embedding'])
            
            if db.db_type == 'postgresql':
                cursor.execute("""
                    INSERT INTO pdf_contents 
                    (filename, page_number, chunk_text, embedding, added_date)
                    VALUES (%s, %s, %s, %s, NOW())
                """, (filename, chunk['page'], chunk['text'], embedding_json))
            elif db.db_type == 'mysql':
                cursor.execute("""
                    INSERT INTO pdf_contents 
                    (filename, page_number, chunk_text, embedding, added_date)
                    VALUES (%s, %s, %s, %s, NOW())
                """, (filename, chunk['page'], chunk['text'], embedding_json))
            else:  # sqlite
                cursor.execute("""
                    INSERT INTO pdf_contents 
                    (filename, page_number, chunk_text, embedding, added_date)
                    VALUES (?, ?, ?, ?, ?)
                """, (filename, chunk['page'], chunk['text'], embedding_json, datetime.now().isoformat()))
        
        conn.commit()
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å®Œäº†: {filename}")
        
        # ç”»åƒã‚’æŠ½å‡ºã—ã¦ä¿å­˜
        try:
            images = extract_images_from_pdf(pdf_path, filename)
            if images:
                save_images_to_db(images)
        except Exception as img_error:
            print(f"âš ï¸ ç”»åƒå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ã¯ç¶™ç¶šï¼‰: {img_error}")
        
        return {
            'filename': filename,
            'page_count': len(pages_text),
            'total_chars': total_chars,
            'total_chunks': len(all_chunks)
        }
        
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        cursor.close()
        conn.close()


def extract_images_from_pdf(pdf_path, filename):
    """PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡º"""
    import pdfplumber
    from PIL import Image
    import io
    
    # ç”»åƒä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    images_dir = os.path.join('assets', 'images', 'pdf_images')
    os.makedirs(images_dir, exist_ok=True)
    
    print(f"ğŸ–¼ï¸ ç”»åƒæŠ½å‡ºé–‹å§‹: {filename}")
    
    extracted_images = []
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                # ãƒšãƒ¼ã‚¸å†…ã®ç”»åƒã‚’æŠ½å‡º
                if hasattr(page, 'images') and page.images:
                    for img_index, img_info in enumerate(page.images, 1):
                        try:
                            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                            if hasattr(page, 'extract_image'):
                                # pdfplumber 0.9.0ä»¥é™
                                image_obj = page.within_bbox(
                                    (img_info['x0'], img_info['top'], 
                                     img_info['x1'], img_info['bottom'])
                                ).to_image()
                                
                                # ç”»åƒã‚’ä¿å­˜
                                base_filename = os.path.splitext(filename)[0]
                                safe_filename = "".join(c for c in base_filename if c.isalnum() or c in (' ', '-', '_'))
                                image_filename = f"{safe_filename}_page{page_num}_img{img_index}.png"
                                image_path = os.path.join(images_dir, image_filename)
                                
                                image_obj.save(image_path)
                                
                                # ç”»åƒæƒ…å ±ã‚’è¨˜éŒ²
                                extracted_images.append({
                                    'filename': filename,
                                    'page_number': page_num,
                                    'image_path': os.path.join('assets', 'images', 'pdf_images', image_filename),
                                    'image_index': img_index,
                                    'width': int(img_info['width']),
                                    'height': int(img_info['height']),
                                    'added_date': datetime.now().isoformat()
                                })
                                
                                print(f"  âœ“ ãƒšãƒ¼ã‚¸{page_num} ç”»åƒ{img_index}ã‚’æŠ½å‡º")
                        except Exception as e:
                            print(f"  âš ï¸ ãƒšãƒ¼ã‚¸{page_num} ç”»åƒ{img_index}ã®æŠ½å‡ºå¤±æ•—: {e}")
                            continue
    except Exception as e:
        print(f"âŒ PDFç”»åƒæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return []
    
    print(f"âœ… {len(extracted_images)}å€‹ã®ç”»åƒã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    return extracted_images


def save_images_to_db(images):
    """ç”»åƒæƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    if not images:
        return
    
    conn = db.get_connection()
    cursor = conn.cursor()
    
    try:
        for img in images:
            if db.db_type == 'postgresql':
                cursor.execute("""
                    INSERT INTO pdf_images 
                    (filename, page_number, image_path, image_index, width, height, added_date)
                    VALUES (%s, %s, %s, %s, %s, %s, NOW())
                """, (
                    img['filename'],
                    img['page_number'],
                    img['image_path'],
                    img['image_index'],
                    img['width'],
                    img['height']
                ))
            elif db.db_type == 'mysql':
                cursor.execute("""
                    INSERT INTO pdf_images 
                    (filename, page_number, image_path, image_index, width, height, added_date)
                    VALUES (%s, %s, %s, %s, %s, %s, NOW())
                """, (
                    img['filename'],
                    img['page_number'],
                    img['image_path'],
                    img['image_index'],
                    img['width'],
                    img['height']
                ))
            else:  # sqlite
                cursor.execute("""
                    INSERT INTO pdf_images 
                    (filename, page_number, image_path, image_index, width, height, added_date)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    img['filename'],
                    img['page_number'],
                    img['image_path'],
                    img['image_index'],
                    img['width'],
                    img['height'],
                    img['added_date']
                ))
        
        conn.commit()
        print(f"âœ… {len(images)}å€‹ã®ç”»åƒæƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜")
    except Exception as e:
        conn.rollback()
        print(f"âŒ ç”»åƒæƒ…å ±ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        raise e
    finally:
        cursor.close()
        conn.close()


def get_images_for_page(filename, page_number):
    """ç‰¹å®šãƒšãƒ¼ã‚¸ã®ç”»åƒã‚’å–å¾—"""
    conn = db.get_connection()
    
    try:
        if db.db_type == 'postgresql':
            cursor = conn.cursor()
            cursor.execute("""
                SELECT image_path, image_index, width, height
                FROM pdf_images
                WHERE filename = %s AND page_number = %s
                ORDER BY image_index
            """, (filename, page_number))
            
            columns = ['image_path', 'image_index', 'width', 'height']
            results = [dict(zip(columns, row)) for row in cursor.fetchall()]
        elif db.db_type == 'mysql':
            cursor = conn.cursor(pymysql.cursors.DictCursor)
            cursor.execute("""
                SELECT image_path, image_index, width, height
                FROM pdf_images
                WHERE filename = %s AND page_number = %s
                ORDER BY image_index
            """, (filename, page_number))
            
            results = [dict(row) for row in cursor.fetchall()]
        else:  # sqlite
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("""
                SELECT image_path, image_index, width, height
                FROM pdf_images
                WHERE filename = ? AND page_number = ?
                ORDER BY image_index
            """, (filename, page_number))
            
            results = [dict(row) for row in cursor.fetchall()]
        
        cursor.close()
        return results
    finally:
        conn.close()


def chunk_text(text, max_chunk_size=1000, overlap=200):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + max_chunk_size
        chunk = text[start:end]
        
        if end < len(text):
            last_period = chunk.rfind('ã€‚')
            last_newline = chunk.rfind('\n')
            last_space = chunk.rfind(' ')
            
            split_point = max(last_period, last_newline, last_space)
            if split_point > max_chunk_size * 0.5:
                chunk = chunk[:split_point + 1]
                end = start + split_point + 1
        
        if chunk.strip():
            chunks.append(chunk.strip())
        
        start = end - overlap
    
    return chunks


def create_embedding(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding


# ============================================
# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•
# ============================================
try:
    print("ğŸš€ RAGå­¦ç¿’ã‚¢ãƒ—ãƒªèµ·å‹•ä¸­...")
    initialize()
    print("âœ… åˆæœŸåŒ–å®Œäº†")
except Exception as e:
    print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    debug_mode = os.environ.get('FLASK_ENV') != 'production'
    
    print(f"ğŸ“± ãƒãƒ¼ãƒˆ {port} ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½")
    app.run(host='0.0.0.0', port=port, debug=debug_mode)